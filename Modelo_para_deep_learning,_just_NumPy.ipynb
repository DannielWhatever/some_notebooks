{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOC+oY4Pps2xsCJOxTAZEmF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DannielWhatever/some_notebooks/blob/main/Modelo_para_deep_learning%2C_just_NumPy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo para deep learning, just NumPy"
      ],
      "metadata": {
        "id": "PgppT0eR-fgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funciones utilitarias  \n",
        "\n",
        "- debug\n",
        "- weights_initialization: Para inicializar los pesos (Kaiming He).\n",
        "- scaler: Para normalizar los datos (basado en fit_transform de sklearn)\n",
        "- dataset_split: Para divivir los datos en train y test (también basada en una función de sklearn)."
      ],
      "metadata": {
        "id": "mTahxmdC7sWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "DEBUG = True\n",
        "\n",
        "def debug(str):\n",
        "  if DEBUG:\n",
        "    print(str)\n",
        "\n",
        "def weights_initialization(out_features, in_features):\n",
        "  \"\"\"\n",
        "  Función para inicializar los pesos de una capa.\n",
        "  \"\"\"\n",
        "  return np.random.randn(out_features, in_features) * np.sqrt(2 / in_features)\n",
        "\n",
        "def scaler(X):\n",
        "  \"\"\"\n",
        "  Función para normalizar los datos.\n",
        "  \"\"\"\n",
        "  mean = np.mean(X, axis=0)\n",
        "  std = np.std(X, axis=0)\n",
        "  return (X - mean) / std\n",
        "\n",
        "def dataset_split(X, y, test_size=0.2):\n",
        "  \"\"\"\n",
        "  Función para dividir los datos en train y test.\n",
        "  \"\"\"\n",
        "  new_indexes = np.arange(len(X))\n",
        "  np.random.shuffle(new_indexes)\n",
        "\n",
        "  X = X[new_indexes]\n",
        "  y = y[new_indexes]\n",
        "\n",
        "  idx = actual_test_size = int(len(X) * test_size)\n",
        "\n",
        "  X_train, X_test = X[:-idx], X[-idx:]\n",
        "  y_train, y_test = y[:-idx], y[-idx:]\n",
        "\n",
        "  return X_train, X_test, y_train, y_test\n"
      ],
      "metadata": {
        "id": "-aqv1jI37s01"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clase Model   \n",
        "\n",
        "Constructor(layers, loss_function)\n",
        "Métodos:\n",
        "- forward\n",
        "- backward\n",
        "- fit\n",
        "- zero_grad\n",
        "- evaluate\n",
        "- summary\n",
        "\n"
      ],
      "metadata": {
        "id": "EMP4JMPB8kN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "  \"\"\"\n",
        "  Clase que representa un modelo de red neuronal.\n",
        "  \"\"\"\n",
        "  def __init__(self, layers, loss_function):\n",
        "    self.layers = layers\n",
        "    self.loss_function = loss_function\n",
        "    self.training = False\n",
        "\n",
        "  def forward(self, X):\n",
        "    debug(\"Model.forward\")\n",
        "    for layer in self.layers:\n",
        "      if hasattr(layer, 'training'):\n",
        "        layer.training = self.training\n",
        "      X = layer.forward(X)\n",
        "    return X\n",
        "\n",
        "  def backward(self, grad_output):\n",
        "    debug(\"Model.backward\")\n",
        "    for layer in reversed(self.layers):\n",
        "      grad_output = layer.backward(grad_output)\n",
        "    return grad_output\n",
        "\n",
        "  def fit(self, X, y, epochs, learning_rate, optimizer_class, print_every=1000):\n",
        "    debug(\"Model.fit\")\n",
        "    start_time = time.time()\n",
        "    self.training = True\n",
        "    optimizer = optimizer_class(self, learning_rate)\n",
        "    for epoch in range(epochs):\n",
        "      debug(f\"Epoch {epoch}\")\n",
        "      self.zero_grad()\n",
        "      output = self.forward(X)\n",
        "      debug(f\"output shape: {output.shape}\")\n",
        "      debug(f\"y shape: {y.shape}\")\n",
        "      loss, grad_loss = self.loss_function(output, y)\n",
        "      self.backward(grad_loss)\n",
        "      optimizer.step()\n",
        "      if epoch % print_every == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "    self.training = False\n",
        "    print(f\"Training complete. Time elapsed: {(time.time() - start_time):.2f} seconds\")\n",
        "\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for layer in self.layers:\n",
        "      if hasattr(layer, 'zero_grad'):\n",
        "        layer.zero_grad()\n",
        "\n",
        "  def evaluate(self, X, y):\n",
        "    output = self.forward(X)\n",
        "    loss, _ = self.loss_function(output, y, calculate_grad=False)\n",
        "    return loss, output\n",
        "\n",
        "  def summary(self):\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      if hasattr(layer, 'weights'):\n",
        "        print(f\"Layer {i + 1}: {layer.__class__.__name__}. Params: {layer.weights.shape}\")\n",
        "      else:\n",
        "        print(f\"Layer {i + 1}: {layer.__class__.__name__}\")"
      ],
      "metadata": {
        "id": "YhzE8YoO8nzl"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layers disponibles\n",
        "\n",
        "- LinearLayer\n",
        "- ReLU\n",
        "- Dropout\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qdgltb5d9Lke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearLayer:\n",
        "  \"\"\"\n",
        "  Clase que representa una capa lineal en una red neuronal.\n",
        "  \"\"\"\n",
        "  def __init__(self, in_features, out_features, bias=True):\n",
        "    self.weights = weights_initialization(out_features, in_features)\n",
        "    self.bias = np.random.randn(out_features) if bias else None\n",
        "    self.grad_weights = np.zeros_like(self.weights)\n",
        "    self.grad_bias = np.zeros_like(self.bias) if bias else None\n",
        "    self.X = None\n",
        "\n",
        "  def forward(self, X):\n",
        "    debug(\" LinearLayer.forward\")\n",
        "    debug(f\"    X shape: {X.shape}\")\n",
        "    debug(f\"    weights shape: {self.weights.shape}\")\n",
        "    self.X = X\n",
        "    output = np.dot(X, self.weights.T)\n",
        "    if self.bias is not None:\n",
        "      output += self.bias\n",
        "    debug(f\"    output shape: {output.shape}\")\n",
        "    return output\n",
        "\n",
        "  def backward(self, grad_output):\n",
        "    debug(\" LinearLayer.backward\")\n",
        "    debug(f\"    grad_output shape: {grad_output.shape}\")\n",
        "    debug(f\"    self.weights shape: {self.weights.shape}\")\n",
        "    debug(f\"    self.X shape: {self.X.shape}\")\n",
        "    self.grad_weights = np.dot(grad_output.T, self.X)\n",
        "    if self.bias is not None:\n",
        "        self.grad_bias = np.sum(grad_output, axis=0)\n",
        "    grad_input = np.dot(grad_output, self.weights)\n",
        "    return grad_input\n",
        "\n",
        "  def get_params(self):\n",
        "    return {'weights': self.weights, 'grad_weights': self.grad_weights, 'bias': self.bias, 'grad_bias': self.grad_bias}\n",
        "\n",
        "  def get_params(self):\n",
        "    return {\n",
        "        'weights': self.weights,\n",
        "        'grad_weights': self.grad_weights,\n",
        "        'bias': self.bias,\n",
        "        'grad_bias': self.grad_bias\n",
        "        }\n",
        "\n",
        "  def zero_grad(self):\n",
        "    if self.grad_weights is not None:\n",
        "      self.grad_weights = np.zeros_like(self.weights)\n",
        "    if self.bias is not None and self.grad_bias is not None:\n",
        "      self.grad_bias = np.zeros_like(self.bias)\n",
        "\n",
        "\n",
        "class ReLU:\n",
        "  \"\"\"\n",
        "  Clase que representa la función de activación ReLU.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "\n",
        "  def forward(self, X):\n",
        "    debug(\" ReLU.forward\")\n",
        "    debug(f\"    X shape: {X.shape}\")\n",
        "    self.input = X\n",
        "    output = np.maximum(0, X)\n",
        "    debug(f\"    output shape: {output.shape}\")\n",
        "    return output\n",
        "\n",
        "  def backward(self, grad_output):\n",
        "    debug(\" ReLU.backward\")\n",
        "    relu_grad = self.input > 0  # Derivada de ReLU: 1 si input > 0, 0 en caso contrario\n",
        "    return grad_output * relu_grad  # Element-wise multiplicación por el gradiente de salida\n",
        "\n",
        "\n",
        "class Dropout:\n",
        "  \"\"\"\n",
        "  Clase que representa la capa de Dropout.\n",
        "  \"\"\"\n",
        "  def __init__(self, dropout_rate):\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.mask = None\n",
        "    self.training = True  # Para indicar si estamos en modo entrenamiento o inferencia\n",
        "\n",
        "  def forward(self, X):\n",
        "    debug(\" Dropout.forward\")\n",
        "    if self.training:\n",
        "        self.mask = (np.random.rand(*X.shape) > self.dropout_rate) / (1.0 - self.dropout_rate)\n",
        "        output = X * self.mask\n",
        "    else:\n",
        "        output = X\n",
        "    debug(f\"    X shape: {X.shape}\")\n",
        "    debug(f\"    output shape: {output.shape}\")\n",
        "    return output\n",
        "\n",
        "  def backward(self, grad_output):\n",
        "    debug(\" Dropout.backward\")\n",
        "    return grad_output * self.mask  # Aplicar la máscara a los gradientes durante el backpropagation\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KXkEZnMe9XPe"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funciones de pérdida\n",
        "\n",
        "- MSELoss\n",
        "\n"
      ],
      "metadata": {
        "id": "MmM6cruU9kHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MSELoss:\n",
        "  \"\"\"\n",
        "  Clase que representa la función de pérdida MSE.\n",
        "  \"\"\"\n",
        "  def __call__(self, y_pred, y_true, calculate_grad=True):\n",
        "    debug(\" MSELoss\")\n",
        "    debug(f\"    Shape y_pred: {y_pred.shape}\")\n",
        "    debug(f\"    Shape y_true: {y_true.shape}\")\n",
        "    grad_loss = None\n",
        "    loss = np.mean((y_pred - y_true) ** 2)\n",
        "    debug(f\"    Loss shape: {loss.shape}\")\n",
        "    if calculate_grad:\n",
        "      grad_loss = 2 * (y_pred - y_true) / y_true.size\n",
        "      debug(f\"    grad_loss shape: {grad_loss.shape}\")\n",
        "    return loss, grad_loss"
      ],
      "metadata": {
        "id": "d2lAcLt19qv2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizers  \n",
        "  \n",
        "- SimpleOptimizer (Gradient descent)\n",
        "- SGDOptimizer\n",
        "- AdamOptimizer\n",
        "\n"
      ],
      "metadata": {
        "id": "uUW5-dGgvVbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SimpleOptimizer:\n",
        "  \"\"\"\n",
        "  Clase que representa el optimizador base.\n",
        "  \"\"\"\n",
        "  def __init__(self, model, learning_rate=0.01):\n",
        "    self.updatable_layers = model.layers.filter(lambda x: hasattr(x, 'get_params'))\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "  def step(self):\n",
        "    for i, layer in enumerate(self.updatable_layers):\n",
        "      if hasattr(layer, 'get_params'):\n",
        "        params = layer.get_params()\n",
        "        grad_weights = params['grad_weights']\n",
        "        grad_bias = params['grad_bias'] if 'grad_bias' in params else None\n",
        "        layer.weights -= self.learning_rate * grad_weights\n",
        "        if layer.bias is not None:\n",
        "          layer.bias -= self.learning_rate * grad_bias\n",
        "\n",
        "\n",
        "class SGDOptimizer(SimpleOptimizer):\n",
        "  \"\"\"\n",
        "  Clase que representa el optimizador SGD.\n",
        "  \"\"\"\n",
        "  def __init__(self, model, learning_rate=0.01, momentum=0.9):\n",
        "    super().__init__(model, learning_rate)\n",
        "    self.momentum = momentum\n",
        "    self.velocities_weights = []\n",
        "    self.velocities_bias = []\n",
        "    for i, layer in enumerate(self.updatable_layers):\n",
        "      params = layer.get_params()\n",
        "      self.velocities_weights.append(np.zeros_like(params['weights']))\n",
        "      self.velocities_bias.append(np.zeros_like(params['bias']) if 'bias' in params else None)\n",
        "\n",
        "  def calculate_weight(self, layer, velocity_weights):\n",
        "    params = layer.get_params()\n",
        "    return self.momentum * velocity_weights + (1 - self.momentum) * params['grad_weights']\n",
        "\n",
        "  def calculate_bias(self, layer, velocity_bias):\n",
        "    params = layer.get_params()\n",
        "    return self.momentum * velocity_bias + (1 - self.momentum) * params['grad_bias']\n",
        "\n",
        "  def step(self):\n",
        "    for i, layer in enumerate(self.updatable_layers):\n",
        "      layer.weights -= self.learning_rate * self.calculate_weight(layer, self.velocities_weights[i])\n",
        "      if layer.bias is not None:\n",
        "        layer.bias -= self.learning_rate * self.calculate_bias(layer, self.velocities_bias[i])\n",
        "\n",
        "class AdamOptimizer(SimpleOptimizer):\n",
        "  \"\"\"\n",
        "  Clase que representa el optimizador Adam.\n",
        "  \"\"\"\n",
        "  def __init__(self, model, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    super().__init__(model, learning_rate)\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.epsilon = epsilon\n",
        "    self.m_weights = []\n",
        "    self.v_weights = []\n",
        "    self.m_bias = []\n",
        "    self.v_bias = []\n",
        "\n",
        "    for i, layer in enumerate(self.updatable_layers):\n",
        "      params = layer.get_params()\n",
        "      self.m_weights.append(np.zeros_like(params['weights']))\n",
        "      self.v_weights.append(np.zeros_like(params['weights']))\n",
        "      self.m_bias.append(np.zeros_like(params['bias']) if 'bias' in params else None)\n",
        "      self.v_bias.append(np.zeros_like(params['bias']) if 'bias' in params else None)\n",
        "\n",
        "  def step(self):\n",
        "    layer_idx = 0\n",
        "    for i, layer in enumerate(self.updatable_layers):\n",
        "      params = layer.get_params()\n",
        "      self.m_weights[i] = self.beta1 * self.m_weights[i] + (1 - self.beta1) * params['grad_weights']\n",
        "      self.v_weights[i] = self.beta2 * self.v_weights[i] + (1 - self.beta2) * params['grad_weights'] ** 2\n",
        "      self.m_bias[i] = self.beta1 * self.m_bias[i] + (1 - self.beta1) * params['grad_bias']\n",
        "      self.v_bias[i] = self.beta2 * self.v_bias[i] + (1 - self.beta2) * params['grad_bias'] ** 2\n",
        "      m_hat_weights = self.m_weights[i] / (1 - self.beta1)\n",
        "      v_hat_weights = self.v_weights[i] / (1 - self.beta2)\n",
        "      m_hat_bias = self.m_bias[i] / (1 - self.beta1)\n",
        "      v_hat_bias = self.v_bias[i] / (1 - self.beta2)\n",
        "      layer.weights -= self.learning_rate * m_hat_weights / (np.sqrt(v_hat_weights) + self.epsilon)\n",
        "      if layer.bias is not None:\n",
        "        layer.bias -= self.learning_rate * m_hat_bias / (np.sqrt(v_hat_bias) + self.epsilon)\n",
        ""
      ],
      "metadata": {
        "id": "DkninPXKIggf"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilizando el modelo"
      ],
      "metadata": {
        "id": "4q_n9hQ6-YwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cargando y preparando los datos\n",
        "\n"
      ],
      "metadata": {
        "id": "CVS8oEBJCKMJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WmPQhpSFCssN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8abb4b77-0524-4f06-e180-f253853f85c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2.1.4)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2024.8.30)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n",
            "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.7\n"
          ]
        }
      ],
      "source": [
        "# https://archive.ics.uci.edu/dataset/320/student+performance\n",
        "!pip install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "DATASET_ID = 320\n",
        "\n",
        "student_performance = fetch_ucirepo(id=DATASET_ID)\n",
        "\n",
        "X = student_performance.data.features\n",
        "y = student_performance.data.targets\n",
        "\n",
        "# Dividir los datos\n",
        "X = X[['studytime', 'failures', 'goout', 'absences', 'Medu', 'Fedu', 'Dalc', 'Walc']]\n",
        "y = y['G3']\n",
        "X = scaler(X.values)\n",
        "X_train, X_test, y_train, y_test = dataset_split(X, y, test_size=0.05)\n",
        "\n",
        "X_train = X_train\n",
        "y_train = y_train.values.reshape(-1, 1)\n",
        "X_test = X_test\n",
        "y_test = y_test.values.reshape(-1, 1)\n",
        "\n",
        "print(f\"X_train. Shape: {str(X_train.shape):<8} Type: {type(X_train)}\")\n",
        "print(f\"y_train. Shape: {str(y_train.shape):<8} Type: {type(y_train)}\")\n",
        "print(f\"X_test.  Shape: { str(X_test.shape):<8} Type: {type(X_test)}\")\n",
        "print(f\"y_test.  Shape: { str(y_test.shape):<8} Type: {type(y_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n15Geh3R2buk",
        "outputId": "b75546d1-3422-4fba-d99d-4323a4407fc9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train. Shape: (617, 8) Type: <class 'numpy.ndarray'>\n",
            "y_train. Shape: (617, 1) Type: <class 'numpy.ndarray'>\n",
            "X_test.  Shape: (32, 8)  Type: <class 'numpy.ndarray'>\n",
            "y_test.  Shape: (32, 1)  Type: <class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define, entrena y prueba el modelo\n"
      ],
      "metadata": {
        "id": "8nxKbSUGCQXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEBUG = False\n",
        "\n",
        "in_params = X_train[0].shape[0]\n",
        "layers = [\n",
        "    LinearLayer(in_features=in_params, out_features=64), ReLU(),\n",
        "    Dropout(0.3), LinearLayer(in_features=64, out_features=32), ReLU(),\n",
        "    Dropout(0.3), LinearLayer(in_features=32, out_features=16), ReLU(),\n",
        "    Dropout(0.3), LinearLayer(in_features=16, out_features=8), ReLU(),\n",
        "    Dropout(0.3), LinearLayer(in_features=8, out_features=3), ReLU(),\n",
        "    LinearLayer(in_features=3, out_features=1)\n",
        "]\n",
        "\n",
        "loss_function = MSELoss()\n",
        "\n",
        "model = Model(layers=layers, loss_function=loss_function)\n",
        "\n",
        "model.summary()\n",
        "print()\n",
        "\n",
        "loss, output = model.evaluate(X_train, y_train)\n",
        "print(f\"\\nRendimiento inicial: {loss}\")\n",
        "\n",
        "myoptimizer = SGDOptimizer\n",
        "model.fit(X_train, y_train, epochs=200000, learning_rate=5*10e-3, optimizer_class=myoptimizer, print_every=10000)\n",
        "\n",
        "loss, output = model.evaluate(X_test, y_test)\n",
        "print(f\"\\nRendimiento final: {loss}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uL4sYS5EPxl8",
        "outputId": "c31d65c7-9823-4608-8eac-1a7f2856751b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1: LinearLayer. Params: (64, 8)\n",
            "Layer 2: ReLU\n",
            "Layer 3: Dropout\n",
            "Layer 4: LinearLayer. Params: (32, 64)\n",
            "Layer 5: ReLU\n",
            "Layer 6: Dropout\n",
            "Layer 7: LinearLayer. Params: (16, 32)\n",
            "Layer 8: ReLU\n",
            "Layer 9: Dropout\n",
            "Layer 10: LinearLayer. Params: (8, 16)\n",
            "Layer 11: ReLU\n",
            "Layer 12: Dropout\n",
            "Layer 13: LinearLayer. Params: (3, 8)\n",
            "Layer 14: ReLU\n",
            "Layer 15: LinearLayer. Params: (1, 3)\n",
            "\n",
            "\n",
            "Rendimiento inicial: 146.48474303134142\n",
            "Epoch 0, Loss: 143.42244469314537\n",
            "Epoch 10000, Loss: 6.7151308591265\n",
            "Epoch 20000, Loss: 5.186779544065626\n",
            "Epoch 30000, Loss: 4.481272698202704\n",
            "Epoch 40000, Loss: 4.490842298020236\n",
            "Epoch 50000, Loss: 3.9469239239994307\n",
            "Epoch 60000, Loss: 4.756128021684299\n",
            "Epoch 70000, Loss: 3.618981372661058\n",
            "Epoch 80000, Loss: 3.6012584889415624\n",
            "Epoch 90000, Loss: 4.0429252666439925\n",
            "Epoch 100000, Loss: 3.7961230273755384\n",
            "Epoch 110000, Loss: 4.115529356709682\n",
            "Epoch 120000, Loss: 3.8188331746398574\n",
            "Epoch 130000, Loss: 3.6289526040161304\n",
            "Epoch 140000, Loss: 3.3146621807052807\n",
            "Epoch 150000, Loss: 3.4480451029782704\n",
            "Epoch 160000, Loss: 3.542528453540287\n",
            "Epoch 170000, Loss: 3.6620332504392534\n",
            "Epoch 180000, Loss: 3.3923097209974955\n",
            "Epoch 190000, Loss: 3.2869810577014555\n",
            "Training complete. Time elapsed: 1037.32 seconds\n",
            "\n",
            "Rendimiento final: 7.974233919751531\n"
          ]
        }
      ]
    }
  ]
}